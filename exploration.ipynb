{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# import scanpath as scp\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "! cp -r /content/drive/MyDrive/scanpath_data /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_data(file):\n",
    "    file_path = os.path.join('/content/scanpath_data/scanpaths/', file)\n",
    "    df = pd.read_csv(file_path, delimiter='\\t')\n",
    "    cols = df.columns.to_list()\n",
    "    df['file'] = file.replace(\"_scanpath.tsv\", \"\")\n",
    "    df = df[['file'] + cols]\n",
    "    return df\n",
    "files = [i for i in os.listdir('/content/scanpath_data/scanpaths/')]\n",
    "master_df = get_file_data(files[0])\n",
    "\n",
    "for i  in  files[1:]:\n",
    "    master_df = pd.concat([master_df, get_file_data(i)])\n",
    "\n",
    "reader_data = pd.read_csv(\"/content/scanpath_data/reader_meta_mean_and_per-text.csv\")\n",
    "\n",
    "def get_scanpath(scan_record):\n",
    "    scanpath = master_df[master_df['file'] == scan_record]\n",
    "    scanpath = scanpath[['fixation_index', 'fixation_duration', 'next_saccade_duration', 'previous_saccade_duration', 'line', 'char_index_in_line',\n",
    "                    'fixation_position_x', 'fixation_position_y', 'word_index_in_text', 'sent_index_in_text', 'char_index_in_text']]\n",
    "    scanpath['change_in_word_flag'] = (scanpath['word_index_in_text'].diff() == 0).astype(int).shift(-1)\n",
    "    scanpath['change_in_word'] = (scanpath['change_in_word_flag'] == 0).cumsum().shift(1).fillna(0)\n",
    "    scanpath['same_word_next_saccade_duration'] = scanpath['change_in_word_flag']*scanpath['next_saccade_duration']\n",
    "    scanpath['next_saccade_duration'] = scanpath['next_saccade_duration'] - scanpath['same_word_next_saccade_duration']\n",
    "    scanpath['fixation_duration'] = scanpath['same_word_next_saccade_duration'] + scanpath['fixation_duration']\n",
    "    scanpath = scanpath.drop('same_word_next_saccade_duration', axis =1)\n",
    "    scanpath = scanpath.groupby(['change_in_word', 'word_index_in_text']).aggregate({'fixation_duration':'sum', 'next_saccade_duration':'sum', 'fixation_position_x':'mean', 'fixation_position_y':'mean'}).reset_index()\n",
    "    scanpath['cum_fixation_duration'] = scanpath['fixation_duration'].cumsum().shift(1).fillna(0)\n",
    "    scanpath = scanpath.reset_index()\n",
    "    return scanpath\n",
    "\n",
    "all_columns = ['file', 'fixation_index', 'text_domain', 'trial', 'acc_bq_1', 'acc_bq_2', 'acc_bq_3', 'acc_tq_1', 'acc_tq_2', 'acc_tq_3',\n",
    "'fixation_duration', 'next_saccade_duration', 'previous_saccade_duration', 'version', 'line', 'roi', 'char_index_in_line',\n",
    "'original_fixation_index', 'is_fixation_adjusted', 'reader_id', 'text_id', 'fixation_position_x', 'fixation_position_y',\n",
    "'word_index_in_text', 'sent_index_in_text', 'char_index_in_text', 'word', 'character', 'text_id_numeric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center_x = 840\n",
    "# center_y = 525\n",
    "# distance = 61\n",
    "# unit = 0.0282\n",
    "\n",
    "# files = master_df['file'].unique()\n",
    "# files = [i for i in files if 'p0' in i]\n",
    "\n",
    "# for i in range(len(files)-1):\n",
    "#     scanpath1 = get_scanpath(files[i])\n",
    "#     scanpath2 = get_scanpath(files[i+1])\n",
    "#     print(files[i], files[i+1])\n",
    "#     score, path, alignment, path_df  = rscasim(scanpath1, scanpath2, center_x, center_y, distance, unit, modulator=0.83)\n",
    "#     # alignment = alignment.drop(0, axis=0).reset_index(drop=True)\n",
    "#     alignment = alignment.merge(scanpath1[['index', 'fixation_position_x', 'fixation_position_y', 'fixation_duration', 'cum_fixation_duration']], left_on='s', right_on='index', how='left').rename(columns={'fixation_position_x':'fixation_position_x_s', 'fixation_position_y':'fixation_position_y_s',                                                                                                                                                                                         'fixation_duration':'fixation_duration_s', 'cum_fixation_duration':'cum_fixation_duration_s'}).drop('index', axis=1)\n",
    "#     alignment = alignment.merge(scanpath2[['index', 'fixation_position_x', 'fixation_position_y', 'fixation_duration', 'cum_fixation_duration']], left_on='t', right_on='index', how='left').rename(columns={'fixation_position_x':'fixation_position_x_t', 'fixation_position_y':'fixation_position_y_t',\n",
    "#                                                                                                                                                                                                             'fixation_duration':'fixation_duration_t', 'cum_fixation_duration':'cum_fixation_duration_t'}).drop('index', axis=1)\n",
    "\n",
    "#     fig, axs = plt.subplots(2, 5, figsize=(8, 12))\n",
    "#     # scanpath1 = scanpath1.iloc[:100, :]\n",
    "#     # scanpath2 = scanpath2.iloc[:100, :]\n",
    "#     axs = plot_scanpaths(axs, [scanpath1, scanpath2])\n",
    "#     axs = plot_alignments(axs, alignment)\n",
    "\n",
    "#     for ax in axs:\n",
    "#         for axx in ax:\n",
    "#             axx.tick_params(axis='x', rotation=90)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.pause(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simmilarity_matrix_df = pd.DataFrame(data = simmilarity_matrix, columns=files, index=files)\n",
    "simmilarity_matrix_df = simmilarity_matrix_df.iloc[:43, :43]\n",
    "files_cluster = [files[i] for i in range(file_count)][:43]\n",
    "simmilarity_matrix_np = simmilarity_matrix_df.to_numpy()\n",
    "from sklearn.cluster import KMeans\n",
    "n_clusters = 2\n",
    "kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)\n",
    "kmeans.fit_predict(simmilarity_matrix_np)\n",
    "cluster_labels = kmeans.labels_.tolist()\n",
    "print(\"Cluster Labels:\", cluster_labels)\n",
    "cluster_df = pd.DataFrame(data=list(zip(files_cluster, cluster_labels)), columns=['file','cluster'])\n",
    "cluster_df['readerID'] = cluster_df['file'].apply(lambda x: int(x.split(\"_\")[0].replace(\"reader\", \"\")))\n",
    "reader_data_ = reader_data[['readerID', 'expert', 'meanPhyAccTQ', 'meanPhyAccBQ']]\n",
    "df = cluster_df.merge(reader_data_, on=\"readerID\").sort_values(by='cluster')\n",
    "# df = df.groupby([\"cluster\", \"expert\"]).agg({\"expert\":\"count\"}).add_suffix(\"_count\").reset_index()\n",
    "# ll = cluster_df[cluster_df['cluster']==0]['file'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clustering = AgglomerativeClustering().fit(simmilarity_matrix_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
